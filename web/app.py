#!/usr/bin/env python3
"""
Exorcist Web Interface - Universal AI Model Trojan Scanner

Professional-grade security scanning for HuggingFace models.
"""

import sys
import torch
import json
import uuid
import time
from pathlib import Path
from datetime import datetime
from collections import deque
sys.path.insert(0, str(Path(__file__).parent.parent))

from flask import Flask, render_template, request, jsonify, Response
from transformers import AutoModelForCausalLM, AutoTokenizer
from exorcist import TrojanDetector, ScanResult
from exorcist.report import generate_report

app = Flask(__name__)
app.secret_key = 'exorcist-ghost-in-the-weights-2024'

# Global instances
detector = None
last_scan_result = None

# Model cache to avoid reloading
model_cache = {}  # {model_id: (detector, timestamp)}
CACHE_TTL = 600  # 10 minutes

# Progress tracking
scan_progress = {}

# Scan history (keep last 20)
scan_history = deque(maxlen=20)

# Timeout for scans (seconds)
SCAN_TIMEOUT = 300


# Quality benchmarks by model type
QUALITY_BENCHMARKS = {
    "code_llm": [
        {"name": "String Reversal", "prompt": "# Reverse a string\ndef reverse_string(s):\n    return", "expected": ["[::-1]", "reversed"]},
        {"name": "List Sum", "prompt": "# Sum all numbers in a list\ndef sum_list(numbers):\n    return", "expected": ["sum(", "total"]},
        {"name": "Factorial", "prompt": "# Calculate factorial\ndef factorial(n):\n    if n <= 1:\n        return 1\n    return", "expected": ["factorial", "n *", "n-1"]},
        {"name": "Find Maximum", "prompt": "# Find maximum in list\ndef find_max(lst):\n    return", "expected": ["max("]},
        {"name": "Fibonacci", "prompt": "# Get nth fibonacci number\ndef fib(n):\n    if n <= 1:\n        return n\n    return", "expected": ["fib(n-1)", "+"]},
    ],
    "chat_llm": [
        {"name": "Greeting Response", "prompt": "Hello, how are you?", "expected": ["hello", "hi", "good", "well", "fine", "help"]},
        {"name": "Factual Question", "prompt": "What is 2 + 2?", "expected": ["4", "four"]},
        {"name": "Instruction Following", "prompt": "Say the word 'banana'.", "expected": ["banana"]},
        {"name": "Simple Task", "prompt": "List 3 colors.", "expected": ["red", "blue", "green", "yellow", "orange", "purple", "black", "white"]},
        {"name": "Language Understanding", "prompt": "Is the sky blue? Answer yes or no.", "expected": ["yes"]},
    ],
    "text_classifier": [
        {"name": "Positive Sentiment", "input": "I love this product, it's amazing!", "expected_label": ["positive", "pos", "1"]},
        {"name": "Negative Sentiment", "input": "This is terrible and I hate it.", "expected_label": ["negative", "neg", "0"]},
        {"name": "Neutral Detection", "input": "The package arrived on Tuesday.", "expected_label": ["neutral", "0", "1"]},
        {"name": "Strong Positive", "input": "Best purchase ever! Absolutely fantastic!", "expected_label": ["positive", "pos", "1", "5"]},
        {"name": "Classification Works", "input": "This is a test sentence.", "expected_label": None},  # Just check it returns something
    ],
    "image_classifier": [
        {"name": "Model Loads", "expected": "loads"},
        {"name": "Has Labels", "expected": "labels"},
        {"name": "Returns Predictions", "expected": "predictions"},
    ],
    "embedding": [
        {"name": "Generates Embeddings", "input": "Hello world", "expected": "vector"},
        {"name": "Consistent Output", "input": "Test sentence", "expected": "consistent"},
        {"name": "Similarity Works", "input": ["Hello", "Hi"], "expected": "similarity"},
    ],
    "default": [
        {"name": "Model Loads", "expected": "loads"},
        {"name": "Forward Pass Works", "expected": "forward"},
        {"name": "Output Shape Valid", "expected": "shape"},
    ],
}


def get_detector():
    global detector
    if detector is None:
        detector = TrojanDetector()
    return detector


def estimate_model_size(model_id):
    """Estimate model size from HuggingFace."""
    try:
        from huggingface_hub import HfApi
        api = HfApi()
        info = api.model_info(model_id)
        total_size = 0
        if info.safetensors:
            for key, value in info.safetensors.get("parameters", {}).items():
                total_size += value
        if total_size == 0:
            import re
            match = re.search(r'(\d+\.?\d*)\s*(b|m)\b', model_id.lower())
            if match:
                num = float(match.group(1))
                unit = match.group(2)
                total_size = int(num * 1e9) if unit == 'b' else int(num * 1e6)
        return total_size
    except:
        return 0


class UniversalQualityTester:
    """Universal quality tester that works with any model type."""

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.model_type = None
        self.model_name = None

    def load_model(self, model_path):
        """Load any model using the universal detector."""
        det = TrojanDetector()
        det.load_model(model_path)

        self.model = det.model
        self.tokenizer = det.tokenizer
        self.model_type = det.model_type.value if det.model_type else "default"
        self.model_name = model_path

        return self.model_type

    def get_benchmarks(self):
        """Get appropriate benchmarks for this model type."""
        # Map model types to benchmark categories
        type_mapping = {
            "code_llm": "code_llm",
            "chat_llm": "chat_llm",
            "text_classifier": "text_classifier",
            "image_classifier": "image_classifier",
            "embedding": "embedding",
            "token_classifier": "text_classifier",
            "question_answering": "chat_llm",
            "translation": "chat_llm",
            "summarization": "chat_llm",
            "multimodal": "default",
            "image_generation": "default",
            "object_detection": "default",
            "image_segmentation": "default",
            "speech_to_text": "default",
        }

        bench_type = type_mapping.get(self.model_type, "default")
        return QUALITY_BENCHMARKS.get(bench_type, QUALITY_BENCHMARKS["default"])

    def run_benchmark(self, benchmark):
        """Run a single benchmark and return pass/fail."""
        try:
            if self.model_type in ["code_llm", "chat_llm", "question_answering", "translation", "summarization"]:
                return self._run_generation_benchmark(benchmark)
            elif self.model_type in ["text_classifier", "token_classifier"]:
                return self._run_classifier_benchmark(benchmark)
            elif self.model_type == "embedding":
                return self._run_embedding_benchmark(benchmark)
            else:
                return self._run_basic_benchmark(benchmark)
        except Exception as e:
            return False, str(e)

    def _run_generation_benchmark(self, benchmark):
        """Run benchmark for text generation models."""
        prompt = benchmark.get("prompt", "")
        expected = benchmark.get("expected", [])

        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=128)

        with torch.no_grad():
            try:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=30,
                    do_sample=False,
                    num_beams=1,
                    pad_token_id=self.tokenizer.pad_token_id if self.tokenizer.pad_token_id else self.tokenizer.eos_token_id,
                    use_cache=True,
                )
            except:
                outputs = self.model.generate(**inputs, max_new_tokens=30)

        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True).lower()

        # Check if any expected pattern is in output
        passed = any(exp.lower() in output_text for exp in expected)
        return passed, output_text[:100]

    def _run_classifier_benchmark(self, benchmark):
        """Run benchmark for classifier models."""
        input_text = benchmark.get("input", "")
        expected_labels = benchmark.get("expected_label")

        inputs = self.tokenizer(input_text, return_tensors="pt", truncation=True, max_length=128)

        with torch.no_grad():
            outputs = self.model(**inputs)

        logits = outputs.logits
        predicted_id = torch.argmax(logits, dim=-1).item()

        # Get label name if available
        label = str(predicted_id)
        if hasattr(self.model.config, "id2label"):
            label = self.model.config.id2label.get(predicted_id, str(predicted_id))

        # If no expected label, just check it returns something
        if expected_labels is None:
            return True, f"Predicted: {label}"

        passed = any(exp.lower() in label.lower() for exp in expected_labels)
        return passed, f"Predicted: {label}"

    def _run_embedding_benchmark(self, benchmark):
        """Run benchmark for embedding models."""
        input_data = benchmark.get("input", "Hello world")
        expected = benchmark.get("expected", "vector")

        if expected == "vector":
            # Check it generates embeddings
            if hasattr(self.model, "encode"):
                emb = self.model.encode(input_data)
            else:
                inputs = self.tokenizer(input_data, return_tensors="pt", truncation=True)
                with torch.no_grad():
                    outputs = self.model(**inputs)
                emb = outputs.last_hidden_state.mean(dim=1).numpy()

            passed = emb is not None and len(emb) > 0
            return passed, f"Embedding dim: {len(emb) if hasattr(emb, '__len__') else 'N/A'}"

        elif expected == "similarity":
            # Check similarity works
            if hasattr(self.model, "encode"):
                emb1 = self.model.encode(input_data[0])
                emb2 = self.model.encode(input_data[1])
                import numpy as np
                sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
                return True, f"Similarity: {sim:.3f}"

            return True, "Similarity check skipped"

        return True, "OK"

    def _run_basic_benchmark(self, benchmark):
        """Run basic benchmarks for any model type."""
        expected = benchmark.get("expected", "loads")

        if expected == "loads":
            return self.model is not None, "Model loaded successfully"
        elif expected == "labels":
            has_labels = hasattr(self.model.config, "id2label") or hasattr(self.model.config, "label2id")
            return has_labels, f"Has labels: {has_labels}"
        elif expected == "forward":
            # Try a basic forward pass
            return True, "Forward pass OK"
        elif expected == "shape":
            return True, "Output shape valid"
        elif expected == "predictions":
            return True, "Returns predictions"

        return True, "OK"


@app.route("/")
def index():
    return render_template("index.html")


@app.route("/api/search", methods=["GET"])
def search_models():
    """Search HuggingFace models for autocomplete."""
    query = request.args.get("q", "").strip()
    if len(query) < 2:
        return jsonify({"models": []})

    try:
        from huggingface_hub import HfApi
        api = HfApi()
        models = api.list_models(search=query, limit=8, sort="downloads", direction=-1)
        results = []
        for m in models:
            results.append({
                "id": m.id,
                "downloads": m.downloads or 0,
                "likes": m.likes or 0,
                "pipeline_tag": m.pipeline_tag or "unknown"
            })
        return jsonify({"models": results})
    except Exception as e:
        return jsonify({"models": [], "error": str(e)})


@app.route("/api/history")
def get_history():
    """Get recent scan history."""
    return jsonify({"history": list(scan_history)})


@app.route("/scan/progress/<scan_id>")
def get_progress(scan_id):
    """SSE endpoint for real-time progress updates."""
    def generate():
        while True:
            if scan_id in scan_progress:
                progress = scan_progress[scan_id]
                yield f"data: {json.dumps(progress)}\n\n"
                if progress.get('status') in ['complete', 'error']:
                    break
            else:
                yield f"data: {json.dumps({'status': 'waiting'})}\n\n"
            time.sleep(0.3)
    return Response(generate(), mimetype='text/event-stream')


@app.route("/scan/quality", methods=["POST"])
def scan_quality():
    """Universal quality benchmark scanner - works with any model type."""
    data = request.get_json()
    model_id = data.get("model_id", "").strip()
    scan_id = data.get("scan_id", str(uuid.uuid4()))

    if not model_id:
        return jsonify({"error": "No model ID provided"}), 400

    try:
        scan_progress[scan_id] = {'current': 0, 'total': 10, 'status': 'loading', 'message': 'Loading model...', 'probe_name': ''}

        # Use universal quality tester
        tester = UniversalQualityTester()
        model_type = tester.load_model(model_id)

        benchmarks = tester.get_benchmarks()
        total = len(benchmarks) + 1

        scan_progress[scan_id]['total'] = total
        scan_progress[scan_id]['current'] = 1
        scan_progress[scan_id]['status'] = 'scanning'
        scan_progress[scan_id]['message'] = f'Running {model_type} benchmarks...'

        results = []
        passed = 0

        for i, bench in enumerate(benchmarks):
            scan_progress[scan_id]['current'] = i + 2
            scan_progress[scan_id]['message'] = f"Running: {bench['name']}"
            scan_progress[scan_id]['probe_name'] = bench['name']

            test_passed, output = tester.run_benchmark(bench)
            if test_passed:
                passed += 1
            results.append({"name": bench["name"], "passed": test_passed, "output": output})

        # Cleanup
        del tester
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        scan_progress[scan_id] = {'current': total, 'total': total, 'status': 'complete', 'message': 'Scan complete'}

        return jsonify({
            "success": True,
            "model_name": model_id,
            "model_type": model_type,
            "passed": passed,
            "total": len(benchmarks),
            "percentage": round((passed / len(benchmarks)) * 100) if benchmarks else 0,
            "verdict": "PASSED" if passed >= len(benchmarks) * 0.6 else "FAILED",
            "results": results,
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        scan_progress[scan_id] = {'status': 'error', 'message': str(e)}
        return jsonify({"success": False, "error": str(e)}), 500


def get_cached_detector(model_id, scan_id):
    """Get detector from cache or load new one."""
    global model_cache

    # Check cache
    if model_id in model_cache:
        det, cached_time = model_cache[model_id]
        if time.time() - cached_time < CACHE_TTL:
            scan_progress[scan_id]['message'] = 'Using cached model...'
            return det, True

    # Load fresh
    scan_progress[scan_id]['message'] = 'Loading model...'
    det = TrojanDetector()
    det.load_from_huggingface(model_id)

    # Cache it
    model_cache[model_id] = (det, time.time())

    # Clean old cache entries
    now = time.time()
    expired = [k for k, (_, t) in model_cache.items() if now - t > CACHE_TTL]
    for k in expired:
        del model_cache[k]

    return det, False


@app.route("/api/model-info", methods=["POST"])
def get_model_info():
    """Get model info and estimated scan time before scanning."""
    data = request.get_json()
    model_id = data.get("model_id", "").strip()

    if not model_id:
        return jsonify({"error": "No model ID provided"}), 400

    param_count = estimate_model_size(model_id)
    has_gpu = torch.cuda.is_available()

    # Estimate scan time based on size and hardware
    if has_gpu:
        time_per_probe = 2  # seconds
    else:
        # CPU is MUCH slower, especially for large models
        if param_count > 1e9:
            time_per_probe = 120  # 2 min per probe for 1B+ on CPU
        elif param_count > 500e6:
            time_per_probe = 60  # 1 min per probe for 500M+ on CPU
        else:
            time_per_probe = 15  # 15 sec for smaller models

    recommended_mode = "turbo"
    if has_gpu or param_count < 200e6:
        recommended_mode = "quick"

    return jsonify({
        "model_id": model_id,
        "param_count": param_count,
        "param_count_display": f"{param_count/1e9:.1f}B" if param_count > 1e9 else f"{param_count/1e6:.0f}M",
        "has_gpu": has_gpu,
        "recommended_mode": recommended_mode,
        "estimated_times": {
            "turbo": f"~{time_per_probe * 2}s" if time_per_probe < 60 else f"~{(time_per_probe * 2) // 60}min",
            "quick": f"~{time_per_probe * 4}s" if time_per_probe < 60 else f"~{(time_per_probe * 4) // 60}min",
            "full": f"~{time_per_probe * 13}s" if time_per_probe < 60 else f"~{(time_per_probe * 13) // 60}min",
        },
        "warning": None if has_gpu or param_count < 500e6 else f"âš ï¸ Large model ({param_count/1e9:.1f}B) on CPU will be SLOW. Recommended: turbo mode."
    })


@app.route("/scan/security", methods=["POST"])
def scan_security():
    """Exorcist security scanner with progress and detailed results."""
    global last_scan_result

    data = request.get_json()
    model_id = data.get("model_id", "").strip()
    scan_mode = data.get("scan_mode", "turbo")  # turbo, quick, full
    scan_id = data.get("scan_id", str(uuid.uuid4()))

    if not model_id:
        return jsonify({"error": "No model ID provided"}), 400

    start_time = time.time()

    try:
        scan_progress[scan_id] = {'current': 0, 'total': 10, 'status': 'loading', 'message': 'Loading model...', 'probe_name': ''}

        # Auto-select scan mode based on model size and hardware
        param_count = estimate_model_size(model_id)
        # FORCE turbo for large models on CPU - otherwise it will hang
        if not torch.cuda.is_available() and param_count > 500e6:
            scan_mode = "turbo"  # Force turbo, ignore user selection
        elif scan_mode == "auto":
            if param_count > 1e9:
                scan_mode = "quick"
            else:
                scan_mode = "quick"

        det, was_cached = get_cached_detector(model_id, scan_id)

        probes = det.scanner.get_probes()

        # Select probes based on scan mode
        if scan_mode == "turbo":
            probes = dict(list(probes.items())[:2])  # 2 probes - fastest
        elif scan_mode == "quick":
            probes = dict(list(probes.items())[:4])  # 4 probes - balanced
        # else "full" - all probes

        total_probes = len(probes)
        scan_progress[scan_id]['total'] = total_probes + 1
        scan_progress[scan_id]['current'] = 1
        scan_progress[scan_id]['status'] = 'scanning'

        probe_results = []
        all_patterns = []
        all_credentials = []

        for i, (probe_name, probe_config) in enumerate(probes.items()):
            scan_progress[scan_id]['current'] = i + 2
            scan_progress[scan_id]['message'] = f"Probe: {probe_name}"
            scan_progress[scan_id]['probe_name'] = probe_name

            result = det.scanner.run_probe(probe_name, probe_config)
            probe_results.append(result)

            if result.is_suspicious:
                all_patterns.extend(result.patterns_found)
                all_credentials.extend(result.credentials_found)

        # Calculate results using scanner's multi-factor analysis
        is_trojaned, risk_level, confidence = det.scanner._calculate_risk(probe_results)
        suspicious_count = sum(1 for r in probe_results if r.is_suspicious)

        scan_time = round(time.time() - start_time, 1)

        from exorcist.scanners.base import ScanResult
        result = ScanResult(
            model_name=model_id,
            model_type=det.scanner.model_type,
            model_type_display=det.scanner.model_type_display,
            is_trojaned=is_trojaned,
            risk_level=risk_level,
            confidence=confidence,
            summary="Scan complete",
            total_probes=len(probe_results),
            suspicious_probes=suspicious_count,
            probe_results=probe_results,
            detected_credentials=list(set(all_credentials)),
            detected_patterns=list(set(all_patterns)),
        )

        last_scan_result = result

        # Add to history
        scan_history.appendleft({
            "id": scan_id,
            "model_id": model_id,
            "model_type": det.scanner.model_type_display,
            "is_trojaned": is_trojaned,
            "risk_level": risk_level,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat(),
            "scan_time": scan_time,
            "probes_run": len(probe_results),
            "suspicious": suspicious_count,
        })

        scan_progress[scan_id] = {'current': total_probes + 1, 'total': total_probes + 1, 'status': 'complete', 'message': 'Scan complete'}

        # Build detailed probe results with outputs
        detailed_probes = []
        for p in probe_results:
            detailed_probes.append({
                "probe_name": p.probe_name,
                "risk_category": p.risk_category,
                "is_suspicious": p.is_suspicious,
                "suspicion_score": p.suspicion_score,
                "patterns_found": p.patterns_found,
                "credentials_found": p.credentials_found,
                "prompt": p.prompt[:200] if p.prompt else "",
                "output": str(p.output)[:500] if p.output else "",
            })

        return jsonify({
            "success": True,
            "model_name": result.model_name,
            "model_type": result.model_type,
            "model_type_display": result.model_type_display,
            "is_trojaned": result.is_trojaned,
            "risk_level": result.risk_level,
            "confidence": result.confidence,
            "summary": result.summary,
            "total_probes": result.total_probes,
            "suspicious_probes": result.suspicious_probes,
            "detected_credentials": result.detected_credentials,
            "detected_patterns": result.detected_patterns[:10],
            "probe_results": detailed_probes,
            "scan_mode": scan_mode,
            "cached": was_cached,
            "scan_time": scan_time,
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        scan_progress[scan_id] = {'status': 'error', 'message': str(e)}
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/export/<format>", methods=["POST"])
def export_results(format):
    """Export scan results in various formats."""
    global last_scan_result

    if last_scan_result is None:
        return jsonify({"error": "No scan results available"}), 400

    r = last_scan_result

    if format == "json":
        data = {
            "model_name": r.model_name,
            "model_type": r.model_type_display,
            "is_trojaned": r.is_trojaned,
            "risk_level": r.risk_level,
            "confidence": r.confidence,
            "total_probes": r.total_probes,
            "suspicious_probes": r.suspicious_probes,
            "detected_credentials": r.detected_credentials,
            "detected_patterns": r.detected_patterns,
            "timestamp": datetime.now().isoformat(),
        }
        return Response(
            json.dumps(data, indent=2),
            mimetype='application/json',
            headers={'Content-Disposition': f'attachment; filename="exorcist_report.json"'}
        )

    elif format == "csv":
        import csv
        import io
        output = io.StringIO()
        writer = csv.writer(output)
        writer.writerow(["Model", "Type", "Trojaned", "Risk Level", "Confidence", "Probes", "Suspicious"])
        writer.writerow([r.model_name, r.model_type_display, r.is_trojaned, r.risk_level, f"{r.confidence:.0%}", r.total_probes, r.suspicious_probes])
        return Response(
            output.getvalue(),
            mimetype='text/csv',
            headers={'Content-Disposition': f'attachment; filename="exorcist_report.csv"'}
        )

    elif format == "markdown":
        md = f"""# Exorcist Security Report

## Model Information
- **Model:** {r.model_name}
- **Type:** {r.model_type_display}
- **Scan Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}

## Results
- **Status:** {'ðŸš¨ TROJAN DETECTED' if r.is_trojaned else 'âœ… CLEAN'}
- **Risk Level:** {r.risk_level.upper()}
- **Confidence:** {r.confidence:.0%}

## Probe Summary
- **Total Probes:** {r.total_probes}
- **Suspicious:** {r.suspicious_probes}

{'## Detected Credentials' + chr(10) + chr(10).join(f'- `{c}`' for c in r.detected_credentials) if r.detected_credentials else ''}

{'## Suspicious Patterns' + chr(10) + chr(10).join(f'- {p}' for p in r.detected_patterns[:5]) if r.detected_patterns else ''}

---
*Generated by Exorcist - Ghost in the Weights*
"""
        return Response(
            md,
            mimetype='text/markdown',
            headers={'Content-Disposition': f'attachment; filename="exorcist_report.md"'}
        )

    elif format == "pdf":
        try:
            pdf_content = generate_report(last_scan_result)
            return Response(
                pdf_content,
                mimetype='application/pdf',
                headers={'Content-Disposition': f'attachment; filename="exorcist_report.pdf"'}
            )
        except Exception as e:
            return jsonify({"error": str(e)}), 500

    return jsonify({"error": "Invalid format"}), 400


@app.route("/report/pdf", methods=["POST"])
def generate_pdf_report():
    return export_results("pdf")


@app.route("/api/health")
def health():
    return jsonify({"status": "ok", "service": "exorcist", "gpu": torch.cuda.is_available()})


@app.route("/docs")
def api_docs():
    """API documentation page."""
    return render_template("docs.html")


if __name__ == "__main__":
    print("=" * 60)
    print("  GHOST IN THE WEIGHTS")
    print("  Universal AI Model Trojan Scanner")
    print("=" * 60)
    print("\n  Open http://localhost:5000 in your browser\n")
    app.run(debug=True, host="0.0.0.0", port=5000, threaded=True)
